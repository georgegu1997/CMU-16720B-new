\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\topmargin -.5in
\textheight 9in
\oddsidemargin -.25in
\evensidemargin -.25in
\textwidth 7in

\newcommand{\code}[1]{\texttt{#1}}

\begin{document}

\author{Gu, Qiao}
\title{16-720B Homework 4 Write-up}
\maketitle

\medskip

\subsection*{Q1.1}

The given translation is applied to all $x_j$ for $j\in \mathbb{R}$. Then for each $x_ii$

\begin{align}
  softmax(x_i+c) = \frac{e^{x_i+c}}{\sum_j e^{x_j+c}} = \frac{e^{x_i}e^c}{\sum_j e^{x_j}e^c}
  = \frac{e^{x_i}e^c}{(\sum_j e^{x_j})e^c} = \frac{e^{x_i}}{\sum_j e^{x_j}} = softmax(x_i).
\end{align}

This shows that softmax is invariant to translation.

When $c=-\max x_i$, all $x_i+c <0$ and $e^{x_i+c} \in (0,1)$, and the difference between $e^{x_I}$ is relatively small. And when $c=0$, $e^{x_i}$ can be exponentially large and may cause numerical instability.

\newpage

\subsection*{Q1.2}

\begin{itemize}
  \item Each element of softmax $softmax(x_i)$ is in range $(0,1)$, and the sum over all elements is $\sum_j softmax(x_j) = 1$.
  \item Probability.
  \item $s_i=e^{x_i}$ is to map each $x_i$ to its probability weight. $S=\sum s_i$ is find the sum of the weights. $softmax(x_I) = \frac{1}{S} x_i$ is to normalize each weight by all weights to get probability.
\end{itemize}

\newpage

\subsection*{Q1.3}

\newcommand{\bx} {\mathbf{x}}
\newcommand{\by} {\mathbf{y}}
\newcommand{\bW} {\mathbf{W}}
\newcommand{\bb} {\mathbf{b}}

Each layer of a neural network can be written mathmatically as $f_i(\bx)=\bW_i\bx+\bb$, and thus is we concatenate $n$ layers together without non-linear layers, we get the output as

\begin{align}
  \by &= f_n(f_{n-1}(\cdots f_1(\bx) \cdots)) \\
  &= \bW_n (\bW_{n-1} (\cdots(\bW_1\bx+\bb_1)\cdots) +\bb_{n-1}) + \bb_n \\
  &= \bW_n \bW_{n-1}\cdots \bW_1 \bx + \bW_n \bW_{n-1}\cdots \bW_2 \bb_1 + \bW_n \bW_{n-1}\cdots \bW_3 \bb_2 + \cdots + \bW_n\bb_{n-1} + \bb_n\\
  &= \bW\bx+b,
\end{align}

where $\bW = \bW_n \bW_{n-1}\cdots \bW_1$ and $\bb = \bW_n \bW_{n-1}\cdots \bW_2 \bb_1 + \bW_n \bW_{n-1}\cdots \bW_3 \bb_2 + \cdots + \bW_n\bb_{n-1} + \bb_n$. This can be regarded as a single linear layer, and the whole network is equivalent to linear regression.

\newpage

\subsection*{Q1.4}

\begin{align}
  \frac{d\sigma(x)}{dx} &= \frac{d}{dx}\frac{1}{1+e^{-x}} \\
  &= (-1)\frac{1}{(1+e^{-x})^2} \frac{d}{dx} (1+e^{-x}) \\
  &= (-1)\frac{1}{(1+e^{-x})^2} (-e^{-x}) \\
  &= \frac{1+e^{-x}-1}{(1+e^{-x})^2} \\
  &- \frac{1}{1+e^{-x}} - \frac{1}{(1+e^{-x})^2} \\
  &= \frac{1}{1+e^{-x}} (1-\frac{1}{1+e^{-x}}) \\
  &= \sigma(x) (1-\sigma(x))
\end{align}

\newpage
\subsection*{Q1.5}

\newcommand{\bdelta} {\mathbf{\delta}}

The loss function is unknown and therefore we assume $\frac{\partial J}{\partial y_i} = \delta_i$. Therefore

\begin{align}
  \frac{\partial J}{\partial W_{ij}} &= \frac{\partial J}{\partial y_i} \frac{\partial y_i}{\partial W_{ij}} = \delta_i x_j \\
  \frac{\partial J}{\partial x_j} &= \sum_{i=0}^k \frac{\partial J}{\partial y_i}\frac{\partial y_i}{\partial x_j} = \delta_i \sum_{i=0}^k W_{ij} \\
  \frac{\partial J}{\partial b_i} &= \frac{\partial J}{\partial y_i} \frac{\partial y_i}{\partial b_i} = \delta_i
\end{align}

And then we can further rewrite it to matrix form

\begin{align}
  \frac{\partial J}{\partial\bW} &= \bdelta \bx^T \\
  \frac{\partial J}{\partial\bx} &= \bW^T \bdelta \\
  \frac{\partial J}{\partial\bb} &= \bdelta
\end{align}

\newpage

\subsection*{Q1.6}

\begin{enumerate}
    \item As shown in Figure.~\ref{fig:q1.6.1}, when the input to the sigmoid $x$ is far away from zero, the magnitude of the gradient becomes very close to zero and thus the gradient from higher layers are scaled by a very small number, even "vanishing". Therefore, when we update the network weights, the changes will be very small.
    \item The output range of sigmoid function is $(0,1)$ and the output range of $tanh(x)$ is $(-1,1)$. If our input data are centered at 0, the output given by $\tanh$ are also centered at 0, which will make the input to different layers consistent.
    \item From Figure.~\ref{fig:q1.6.1}, we can see that $\tanh(x)$ has a stronger gradient (larger magnitude) than $\sigma(x)$ does.
    \item $\tanh(x)=2\sigma(2x)-1$ as
    \begin{align}
    \begin{split}
        \sigma(x) = \frac{1}{1+e^{-x}}
        &\Rightarrow \sigma(2x) = \frac{1}{1+e^{-2x}}
        \Rightarrow 2\sigma(2x) = \frac{2}{1+e^{-2x}} \\
        &\Rightarrow 2\sigma(2x)-1 = \frac{2-1-e^{-2x}}{1+e^{-2x}} = \frac{1-e^{-2x}}{1+e^{-2x}} = \tanh(x)
    \end{split}
    \end{align}
\end{enumerate}

\begin{figure}[h!]
    \centering
    \includegraphics[width=.6\linewidth]{../results/q1_6_1.png}
    \caption{The plot of sigmoid function $\sigma(x)$ (in blue), its gradient $(1-\sigma(x))\sigma(x)$ (in red), the $\tanh(x)$ function (in green) and its gradient $1-\tanh^2(x)$ (in orange). }
    \label{fig:q1.6.1}
\end{figure}

\newpage

\subsection*{Q2.1.1}

If the weights and biases of every layer are initialized with zeros, the outpuut layer pre-activation values will be zeros regardless the input data and the post-activation values will be a uniform probability distribution.
In this case, there will still be gradients in the network due to the softmax loss, but they will be very small. Therefore the update of the network is very slow, the training process may terminate without convergence or the network may be stuck at local optima.
% Therefore The gradients of the network therefore will be zeros during training and the network weights will remain zeros after training.

\newpage

\subsection*{Q2.1.3}

Because if we initialize all the weights with the same number, some of them will have the same effect on the output values, and the gradients on them will be the same during backpropagation. Therefore, they will move together and hard to converge during training.

Because in one layer of neural net $\by=\bW\bx+\bb$, the variance of the output $\by$ depends on both the variance of input $\bx$ and variance of elements of the weight $\bW$. This is also true during the backpropagation of gradients. Therefore we scale the weight variance down by the layer size, To make the output variance of each layer equal to the input variance.




% \begin{figure}
%     \center
%     \includegraphics[width=.8\linewith]{../results/q1_6_1.png}
%     \caption{}
%     \label{}
% \end{figure}

\end{document}
